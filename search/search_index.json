{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#anonllm","title":"AnonLLM","text":"<p>AnonLLM is a software, for researchers, that helps in setting up a repeatable, reproducible,  replicable protocol for training and evaluating multitask LLM for recommendation!</p> <p>Features:</p> <ul> <li>Two different model family implemented at the moment of writing (T5 and GPT2)</li> <li>Fully vectorized Ranking (<code>NDCG</code>, <code>MAP</code>, <code>HitRate</code>, ...) and Error (<code>RMSE</code>, <code>MAE</code>) metrics</li> <li>Fully integrated with WandB monitoring service</li> <li>Full use of transformers and datasets libraries</li> <li>Easy to use (via <code>.yaml</code> configuration or Python api)</li> <li>Fast (Intended to be used for consumer gpus)</li> <li>Fully modular and easily extensible!</li> </ul> <p>The goal of AnonLLM is to be the starting point, a hub, for all developers which want to evaluate the capability  of LLM models in the recommender system domain with a keen eye on devops best practices!</p> <p>Want a glimpse of AnonLLM? This is an example configuration which runs the whole experiment pipeline, starting from data pre-processing, to evaluation:</p> <pre><code>exp_name: to_the_moon\ndevice: cuda:0\nrandom_seed: 42\n\ndata:\n  AmazonDataset:\n    dataset_name: toys\n\nmodel:\n  T5Rec:\n    name_or_path: \"google/flan-t5-base\"\n  n_epochs: 10\n  train_batch_size: 32\n  train_tasks:\n    - SequentialSideInfoTask\n    - RatingPredictionTask\n\neval:\n  eval_batch_size: 16\n  eval_tasks:\n    SequentialSideInfoTask:\n      - hit@1\n      - hit@5\n      - map@5\n    RatingPredictionTask:\n      - rmse\n</code></pre> <p>The whole pipeline can then be executed by simply invoking <code>python anonLLM.py -c config.yml</code>!</p>"},{"location":"#motivation","title":"Motivation","text":"<p>The adoption of LLM in the recommender system domain is a new research area, thus it's difficult to  find pre-made and well-built software designed specifically for LLM.</p> <p>With AnonLLM the idea is to fill that gap, or at least \"start the conversation\" about the importance of developing accountable experiment pipelines</p>"},{"location":"#credits","title":"Credits","text":"<p>A heartfelt \"thank you\" to P5 authors which, with their work, inspired the idea of this repository and for making available a  preprocessed version of the  Amazon Dataset which in this project I've used as starting point  for further manipulation.</p> <p>Yes, the cute logo is A.I. generated. So thank you DALL-E 3! </p>"},{"location":"#project-organization","title":"Project Organization","text":"<pre><code>\u251c\u2500\u2500 \ud83d\udcc1 data                          &lt;- Directory containing all data generated/used\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 processed                     &lt;- The final, canonical data sets used for training/validation/evaluation\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 raw                           &lt;- The original, immutable data dump\n\u2502\n\u251c\u2500\u2500 \ud83d\udcc1 models                        &lt;- Directory where trained and serialized models will be stored\n\u2502\n\u251c\u2500\u2500 \ud83d\udcc1 reports                       &lt;- Where metrics will be stored after performing the evaluation phase\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 metrics                          \n\u2502\n\u251c\u2500\u2500 \ud83d\udcc1 src                           &lt;- Source code of the project\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 data                          &lt;- All scripts related to datasets and tasks\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 datasets                  &lt;- All datasets implemented\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 tasks                     &lt;- All tasks implemented\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 abstract_dataset.py       &lt;- The interface that all datasets should implement\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 abstract_task.py          &lt;- The interface that all tasks should implement\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 main.py                   &lt;- Script used to perform the data phase when using AnonLLM via .yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 evaluate                  &lt;- Scripts to evaluate the trained models\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 metrics                   &lt;- Scripts containing different metrics to evaluate the predictions generated\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 abstract_metric.py        &lt;- The interface that all metrics should implement\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 evaluator.py              &lt;- Script containing the Evaluator class used for performing the eval phase\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 main.py                   &lt;- Script used to perform the eval phase when using AnonLLM via .yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 model                     &lt;- Scripts to define and train models\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc1 models                    &lt;- Scripts containing all the models implemented\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 abstract_model.py         &lt;- The interface that all models should implement\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 main.py                   &lt;- Script used to perform the eval phase when using AnonLLM via .yaml\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 trainer.py                &lt;- Script containing the Trainer class used for performing the train phase\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py               &lt;- Makes src a Python module\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 utils.py                  &lt;- Contains utils function for the project\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 yml_parse.py              &lt;- Script responsible for coordinating the parsing of the .yaml file\n\u2502\n\u251c\u2500\u2500 \ud83d\udcc4 LICENSE                       &lt;- MIT License\n\u251c\u2500\u2500 \ud83d\udcc4 anonLLM.py                   &lt;- Script to invoke via command line to use AnonLLM via .yaml\n\u251c\u2500\u2500 \ud83d\udcc4 params.yml                    &lt;- The example .yaml config for starting using AnonLLM\n\u251c\u2500\u2500 \ud83d\udcc4 README.md                     &lt;- The top-level README for developers using this project\n\u2514\u2500\u2500 \ud83d\udcc4 requirements.txt              &lt;- The requirements file for reproducing the environment (src package)\n</code></pre> <p>Project based on the cookiecutter data science project template. #cookiecutterdatascience</p>"},{"location":"python_api_usage/wip/","title":"Python API usage","text":"<p>Warning</p> <p>This section is a WIP. Sorry about that \ud83e\udd15</p>"},{"location":"quickstart/installation/","title":"Installation","text":""},{"location":"quickstart/installation/#from-source-recommended","title":"From source recommended","text":"<p>AnonLLM requires Python 3.10 or later, and all packages needed are listed in  <code>requirements.txt</code></p> <ul> <li>Torch with cuda 11.7 has been set as requirement for reproducibility purposes, but feel free to change the cuda   version with the most appropriate for your use case!</li> </ul> <p>To install AnonLLM:</p> <ol> <li>Clone this repository:   <pre><code>git clone https://github.com/doubleblind-anon/AnonLLM.git\n</code></pre></li> <li>Install the requirements:   <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Start experimenting!</li> </ol> <p>NOTE: It is highly suggested to set the following environment variables to obtain 100% reproducible results of your experiments:</p> <pre><code>export PYTHONHASHSEED=42\nexport CUBLAS_WORKSPACE_CONFIG=:16:8\n</code></pre> <p>You can check useful info about the above environment variables here and here</p> <p>Info</p> <p>AnonLLM can be easily run by defining a <code>.yaml</code> file, which encapsulates the full logic of the experiment, or by accessing the Python API (a more flexible and powerful approach). Check the <code>.yaml</code> sample example or the <code>python</code> one to get up and running with AnonLLM!</p> <p>Tip: It is suggested to install AnonLLM requirements in a virtual environment</p> <p>Virtual environments are special isolated environments where all the packages and versions you install only  apply to that specific environment. It\u2019s like a private island! \u2014 but for code.</p> <p>Read this Medium article for understanding all the advantages and the official python guide on how to set up one</p>"},{"location":"quickstart/installation/#via-docker-image","title":"Via Docker Image","text":"<p>Simply pull the latest AnonLLM Docker Image  which includes every preliminary step to run the project, including setting <code>PYTHONHASHSEED</code> and <code>CUBLAS_WORKSPACE_CONFIG</code> for reproducibility purposes!</p>"},{"location":"quickstart/sample_experiments/","title":"Sample Experiments","text":"<p>The directory <code>sample_experiments</code> contains all the <code>.yml</code> config files and results of Experiment 2 and Experiment 3: - Experiment 2 aims at assessing, in a reproducible environment thanks to AnonLLM, the impact of the personalization strategy introduced in the P5 paper, by using the same prompts defined in the mentioned paper - Experiment 3 aims at evaluating AnonLLM performances by varying the LLM backbone and use a novel set of more informative prompts. Majority of the runs overcame results of the mentioned P5 paper. The new set of prompts can be found here</p> <p>Each result directory contains a table storing metrics results for each task in both <code>.csv</code> and <code>.tex</code> format, generated with AnonLLM.</p> <p>All runs have been tracked with WandB. The full workspace is available by clicking the following image:</p> <p> </p> <p>Note: additional experiments can be found in the <code>other_exp</code> subfolder!</p>"},{"location":"quickstart/sample_experiments/#experiment-2-results","title":"Experiment 2 results","text":"<p>These are the results of the T5-S LLM, with (+W) and without personalization, when trained and evaluated on the Sequential, Direct and Rating Prediction  tasks with the P5 prompts. The P5 prompts used can be found here</p> <p>The evaluation is carried out on a seen prompt and an unseen one.</p> <p> </p> <ul> <li>T5-S: [.yml config][Results directory][Visualize in WandB]</li> <li>T5-S + W: [.yml config][Results directory][Visualize in WandB]</li> </ul>"},{"location":"quickstart/sample_experiments/#experiment-3-results","title":"Experiment 3 results","text":"<p>These are the results of T5-S, FlanT5-S, FlanT5-B, GPT2, with (+W) and without personalization, when trained and evaluated on the Sequential, Direct and Rating Prediction tasks with the novel set of prompts defined in AnonLLM. The evaluation is carried out on all prompts, already seen by the model during the fine-tuning phase, and in the following table there are reported the best results for each metric achieved by any prompt of the specific task (best-seen).</p> <p> </p>"},{"location":"quickstart/sample_experiments/#t5-runs","title":"T5 Runs","text":"<ul> <li>T5-S: [.yml config][Results directory][Visualize in WandB]</li> <li>T5-S + W: [.yml config][Results directory][Visualize in WandB]</li> </ul>"},{"location":"quickstart/sample_experiments/#flan-t5-runs","title":"Flan T5 Runs","text":"<ul> <li>FlanT5-S: [.yml config][Results directory][Visualize in WandB]</li> <li>FlanT5-S + W: [.yml config][Results directory][Visualize in WandB]</li> <li>FlanT5-B: [.yml config][Results directory][Visualize in WandB]</li> <li>FlanT5-B + W: [.yml config][Results directory][Visualize in WandB]</li> </ul>"},{"location":"quickstart/sample_experiments/#gpt2-runs","title":"GPT2 Runs","text":"<ul> <li>GPT2: [.yml config][Results directory][Visualize in WandB]</li> <li>GPT2 + W: [.yml config][Results directory][Visualize in WandB]</li> </ul>"},{"location":"quickstart/simple_python_example/","title":"Simple Python example","text":"<p>Just like the quickstart example foy .yaml, in this simple experiment we will:</p> <ol> <li>Use the <code>toys</code> Amazon Dataset and add 'item_' and 'user_'     prefixes to each item and user ids</li> <li>Train the GPT2Rec using the <code>distilgpt2</code> checkpoint on the     <code>SequentialSideInfoTask</code></li> <li>Evaluate results using <code>hit@10</code> and <code>hit@5</code> metrics</li> </ol> <p>The model trained will be saved into models/simple_experiment path and the metrics results into reports/metrics/simple_experiment path</p> Run the experiment<pre><code>from src.data.datasets.amazon_dataset import AmazonDataset\nfrom src.data.tasks.tasks import SequentialSideInfoTask\nfrom src.evaluate.evaluator import RecEvaluator\nfrom src.evaluate.metrics.ranking_metrics import Hit\nfrom src.model.models.gpt import GPT2Rec\nfrom src.model.trainer import RecTrainer\n\nif __name__ == \"__main__\":\n\n    # data phase\n    ds = AmazonDataset(\"toys\", add_prefix_items_users=True)\n\n    ds_splits = ds.get_hf_datasets()  # this returns a dict of hf datasets\n\n    train_split = ds_splits[\"train\"]\n    val_split = ds_splits[\"validation\"]\n    test_split = ds_splits[\"test\"]\n\n    # model phase\n    model = GPT2Rec(\"distilgpt2\",\n                    training_tasks_str=[\"SequentialSideInfoTask\"],\n                    all_unique_labels=list(ds.all_items))\n\n    trainer = RecTrainer(model,\n                         n_epochs=10,\n                         batch_size=8,\n                         train_sampling_fn=ds.sample_train_sequence,\n                         output_dir=\"models/simple_experiment\")\n\n    trainer.train(train_split)\n\n    # eval phase\n    evaluator = RecEvaluator(model, eval_batch_size=4)\n\n    evaluator.evaluate_suite(test_split,\n                             tasks_to_evaluate={SequentialSideInfoTask(): [Hit(k=10), Hit(k=5)]},\n                             output_dir=\"reports/metrics/simple_experiment\")\n</code></pre> <p>As you can see, it's very easy to perform a complete experiment also via the Python API!</p>"},{"location":"quickstart/simple_python_example/#reducing-dataset-size-for-testing-purposes","title":"Reducing dataset size for testing purposes","text":"<p>If you really want to  get a glimpse of the final results of the process without having to wait a lot,  since AmazonDataset is quite big, you could cut the datasets size of each split for testing purposes!</p> <p>This is the new data phase in which each dataset split size has been reduced</p> Data phase with reduced dataset size for testing<pre><code>from datasets import Dataset\nfrom src.data.datasets.amazon_dataset import AmazonDataset\n\n# data phase\nds = AmazonDataset(\"toys\", add_prefix_items_users=True)\n\nds_splits = ds.get_hf_datasets()\n\ntrain_split = Dataset.from_dict(ds_splits[\"train\"][:100])\nval_split = Dataset.from_dict(ds_splits[\"validation\"][:100])\ntest_split = Dataset.from_dict(ds_splits[\"test\"][:100])\n</code></pre> <p>As you can see, the full integration with state-of-the-art libraries makes AnonLLM feel like home!</p>"},{"location":"quickstart/simple_yaml_example/","title":"Simple Yaml example","text":"<p>In this simple experiment, we will: 1. Use the <code>toys</code> Amazon Dataset and add <code>item_</code> and <code>user_</code>     prefixes to each item and user ids 2. Train the GPT2Rec using the <code>distilgpt2</code> checkpoint on the     <code>SequentialSideInfoTask</code> 3. Evaluate results using <code>hit@10</code> and <code>hit@5</code> metrics</p> <p>Info</p> <p>Please remember that to invoke AnonLLM using the <code>.yaml</code> configuration, the working directory should be the repository root!</p>"},{"location":"quickstart/simple_yaml_example/#yaml-config","title":"Yaml config","text":"<p>Define your custom <code>params.yml:</code></p> params.yml<pre><code>exp_name: simple_exp\ndevice: cuda:0\nrandom_seed: 42\n\ndata:\n  AmazonDataset:\n    dataset_name: toys\n    add_prefix_items_users: true\n\nmodel:\n  GPT2Rec:\n    name_or_path: \"distilgpt2\"\n  n_epochs: 10\n  train_batch_size: 8\n  train_tasks:\n    - SequentialSideInfoTask\n\neval:\n  eval_batch_size: 4\n  eval_tasks:\n    SequentialSideInfoTask:\n      - hit@10\n      - hit@5\n</code></pre>"},{"location":"quickstart/simple_yaml_example/#invoke-anonllm","title":"Invoke AnonLLM","text":"<p>After defining the above <code>params.yml</code>, simply execute the experiment with </p> Run the experiment<pre><code>python anonLLM.py -c params.yml\n</code></pre> <p>The model trained and the evaluation results will be saved into <code>models</code> and <code>reports/metrics</code></p>"},{"location":"yaml_usage/introduction/","title":"Yaml usage","text":"<p>Using AnonLLM via <code>.yaml</code> is really simple:</p> <ul> <li>You define the steps of your experiment following the data-model-evaluation logic</li> <li>You invoke AnonLLM with the following command:</li> </ul> <pre><code>python AnonLLM.py -c params.yml # (1)\n</code></pre> <ol> <li>Instead of params.yml, specify the path to your .yaml file</li> </ol> <p>Info</p> <p>The necessary requirement for using AnonLLM with the user-defined .yaml configuration is to set the root of the repository as the Working Directory</p>"},{"location":"yaml_usage/introduction/#yaml-interface","title":"Yaml interface","text":"<p>The .yaml file parameters can be grouped into four different macro-section:</p> <ol> <li>The first section contains general parameters needed by all the other sections, like     the experiment name, the random state, the device to use, etc.</li> <li>The second section contains all the parameters needed by the data phase of the experiment,     such as the dataset to use and its parameters</li> <li>The third section contains all the parameters needed by the model phase of the experiment,     such as the model to use, the number of epochs to train the model, the train batch size, etc.</li> <li>The fourth section contains all the parameters needed for the eval phase of the experiment,     such as the eval batch size, the metrics to use, etc.</li> </ol> <p>You can check each subsection to see all its customizable parameters!</p>"},{"location":"yaml_usage/available_implementations/available_datasets/","title":"Available Datasets","text":""},{"location":"yaml_usage/available_implementations/available_datasets/#amazondataset","title":"AmazonDataset","text":"<p>The Amazon Dataset is a processed implementation of the Amazon Dataset provided by the author's of  the P5 Paper. There are three available splits (beauty, sports, toys) with the following statistics (image taken from the mentioned P5 paper):</p> <p></p> <p>For each user, the following information is available:</p> <ul> <li>The sequence of bought items</li> <li>The rating assigned to each item</li> </ul> <p>For each item, the following information is available:</p> <ul> <li>Description</li> <li>Categories</li> <li>Title</li> <li>Price</li> <li>Image URL</li> <li>Brand</li> </ul> AmazonDataset parameters<pre><code>AmazonDataset:\n\n  # The Amazon dataset split to use. Can be 'beauty', 'sports', 'toys'\n  #\n  # Required\n  dataset_name: toys\n\n  # If set to true, users and items ids will have \"user_\" and \"item_\" prefix in the dataset\n  # E.g.:\n  # add_prefix_items_users: false -&gt; users = [1, 2, 3, ...]; items = [1, 2, 3, ...]\n  # add_prefix_items_users: true -&gt; users = [user_1, user_2, user_3, ...]; items = [item_1, item-2, item_3, ...]\n  #\n  # Optional, Default: true\n  add_prefix_items_users: true\n\n  # If set to true item ids will start from 1001 rather than 1  # (1)\n  # E.g.:\n  # items_start_from_1001: false -&gt; items = [1, 2, 3, ...]\n  # items_start_from_1001: true -&gt; items = [1001, 1002, 1003, ...]\n  items_start_from_1001: false\n</code></pre> <ol> <li>This is to fully exploit the LLM model tokenization: with sequential indexing, items with similar id should have    more importance, thus by starting item ids from 1001 rather than 1 the sentencepiece tokenizer will     tokenize with same subtokens items with similar ids!    For more details check the following paper</li> </ol>"},{"location":"yaml_usage/available_implementations/available_metrics/","title":"Available Metrics","text":"<p>To specify one of the following metrics as metric in the <code>.yaml</code> file, you can simply use its name  (The parsing is not case-sensitive)!</p> <p>For example, to use  and :</p> Example using Hit and MAP@10<pre><code>...\neval:\n  SequentialSideInfoTask:\n    - hit\n    - map@10\n    ...\n</code></pre>"},{"location":"yaml_usage/available_implementations/available_metrics/#ranking-metrics","title":"Ranking Metrics","text":"<p>Info</p> <p>Each Ranking Metric can evaluate recommendations produced with a cut-off value: To specify it, simply use <code>METRIC_NAME@K</code></p>"},{"location":"yaml_usage/available_implementations/available_metrics/#hit-hitk","title":"Hit (Hit@K)","text":"<p>The Hit metric simply check if, for each user, at least one relevant item (that is, an item present in the ground truth of the user) has been recommended.</p> <p>In math formulas, the Hit for the single user is computed like this:</p> <p> </p> <p>Where:</p> <ul> <li>  is the recommendation list for user  </li> <li>  is the ground truth for user  </li> </ul> <p>And the Hit for the whole model is basically the average Hit for each user:</p> <p> </p> <p>Where:</p> <ul> <li>  is the set containing all users</li> </ul>"},{"location":"yaml_usage/available_implementations/available_metrics/#map-mapk","title":"MAP (MAP@K)","text":"<p>The  metric (Mean average Precision) is a ranking metric computed by first calculating the  (Average Precision) for each user and then taking the average.</p> <p>The  is calculated as such for the single user:</p> <p> </p> <p>Where:</p> <ul> <li>  is the number of relevant items for the user  </li> <li>  is the number of recommended items for the user  </li> <li>  is the precision computed at cutoff  </li> <li>  is a binary function defined as such:</li> </ul> <p> </p> <p>After computing the  for each user, the  can be computed for the whole model:</p> <p> </p>"},{"location":"yaml_usage/available_implementations/available_metrics/#mrr-mrrk","title":"MRR (MRR@K)","text":"<p>The  (Mean Reciprocal Rank) computes, for each user, the inverse position of the first relevant item (that is, an item present in the ground truth of the user), and then the average is computed to obtain a model-wise metric.</p> <p>In math formulas:</p> <p> </p> <p>Where:</p> <ul> <li>  is the set containing all users</li> <li>  is the position of the first relevant item in the recommendation list of the  user</li> </ul>"},{"location":"yaml_usage/available_implementations/available_metrics/#ndcg-ndcgk","title":"NDCG (NDCG@K)","text":"<p>The  (Normalized Discounted Cumulative Gain) metric compares the actual ranking with the ideal one. First, the  is computed for each user:</p> <p> </p> <p>Where:</p> <ul> <li>  is the recommendation list for user  </li> <li>  is a binary function defined as such:</li> </ul> <p> </p> <p>Then the  for a single user is calculated using the following formula:</p> <p> </p> <p>Where:</p> <ul> <li>  is the  sorted in descending order (representing the ideal ranking)</li> </ul> <p>Finally, the  of the whole model is calculated averaging the  of each user:</p> <p> </p> <p>Where:</p> <ul> <li>  is the set containing all users</li> </ul>"},{"location":"yaml_usage/available_implementations/available_metrics/#error-metrics","title":"Error Metrics","text":"<p>Error metrics calculate the error the model made in predicting the rating a particular user would have  given to an unseen item</p>"},{"location":"yaml_usage/available_implementations/available_metrics/#mae","title":"MAE","text":"<p>The  (Mean Absolute Error) computes, in absolute value, the difference between rating predicted and actual rating:</p> <p> </p> <p>Where:</p> <ul> <li>  is the set containing all users</li> <li>  is the actual score give by user  to item  </li> <li>  is the predicted score give by user  to item  </li> </ul>"},{"location":"yaml_usage/available_implementations/available_metrics/#rmse","title":"RMSE","text":"<p>The  (Root Mean Squared Error) computes the difference, squared, between rating predicted and actual rating:</p> <p> </p> <p>Where:</p> <ul> <li>  is the set containing all users</li> <li>  is the actual score give by user  to item  </li> <li>  is the predicted score give by user  to item  </li> </ul>"},{"location":"yaml_usage/available_implementations/available_models/","title":"Available models","text":""},{"location":"yaml_usage/available_implementations/available_models/#t5rec","title":"T5Rec","text":"<p>This model implements T5 for the recommendation setting. It is implemented by using the HuggingFace library, thus you can pass to the model any parameters that you would  pass to the T5Config and to the GenerationConfig.</p> <p>Info</p> <p>Some GenerationConfig parameters have default value for T5Rec:</p> <ul> <li><code>num_return_sequences = 10</code></li> <li><code>num_beams = 30</code></li> <li><code>no_repeat_ngram_size = 0</code></li> <li><code>early_stopping = True</code></li> </ul> <p>Remember: For non-ranking task, <code>num_return_sequences</code> will be set to 1 when generating predictions regardless of what you set in the .yaml file</p> T5Rec<pre><code>T5Rec:\n\n\n  # The checkpoint that should be used as starting point of the\n  # fine-tuning process. It can be a model name hosted at hugging face\n  # or a local path # (1)\n  # \n  # Required\n  name_or_path: \"google/flan-t5-small\"\n\n  # If set to true, this adds an EmbeddingLayer to the model which tries to encode user information\n  # from their ids, and project the encoded representation in the hidden dimension space of the\n  # chosen model. The user encoded information is then summed to the encoded information of each token in the\n  # input prompt, with the idea of \"translating\" the encoded input to a specific region in the latent space.\n  # This sum is later passed to the forward method of the T5 model\n  #\n  # Optional, Default: false\n  inject_user_embeds: false\n\n  # If set to true, this adds a custom EmbeddingLayer to the model which encodes whole word information.\n  # This encoding produces embeddings which have same hidden dimension of the input embeddings, and the two\n  # are summed together. # (2)\n  #\n  # Optional, Default: false\n  inject_whole_word_embeds: false\n\n  # You can pass any parameter that you would pass to the T5Config when instantiating the model with the\n  # HuggingFace library # (3)\n  CONFIG_PARAM_1: CONFIG_VAL_1\n  CONFIG_PARAM_2: CONFIG_VAL_2\n  ... \n\n  # You can pass any parameter that you would pass to the GenerationConfig when instantiating it with the\n  # HuggingFace library # (4)\n  GEN_PARAM_1: GEN_VAL_1\n  GEN_PARAM_2: GEN_VAL_2\n  ...\n</code></pre> <ol> <li>Check all the available models hosted at HuggingFace!</li> <li>This is based on the architecture of the P5 model described in this research paper</li> <li>Check all the config parameters that you can pass from the HuggingFace    official documentation</li> <li>Check all the generation parameters that you can pass from the HuggingFace    official documentation</li> </ol> <p>This is a visualization of what <code>inject_user_embeds</code> set to <code>true</code> does:</p> <p></p> <p>This is a visualization of what <code>inject_whole_word_embeds</code> set to <code>true</code> does:</p> <p></p>"},{"location":"yaml_usage/available_implementations/available_models/#gpt2rec","title":"GPT2Rec","text":"<p>This model implements GPT2 for the recommendation setting. It is implemented by using the HuggingFace library, thus you can pass to the model any parameters that you would  pass to the GPT2Config and to the GenerationConfig.</p> <p>Since GPT2 is a text-generation model, input text and target text of the specific task are \"merged\" into a  single prompt with prefix Input: and Target: respectively</p> <p>Info</p> <p>Some GenerationConfig parameters have default value for GPT2Rec:</p> <ul> <li><code>num_return_sequences = 10</code></li> <li><code>max_length = TOKENIZER_MAX_LENGTH</code></li> <li><code>no_repeat_ngram_size = 0</code></li> <li><code>early_stopping = True</code></li> </ul> <p>Remember:</p> <ul> <li>For non-ranking task, <code>num_return_sequences</code> will be set to 1 when generating predictions regardless   of what you set in the .yaml file</li> <li><code>max_length</code> is set to the tokenizer max length since, when performing generation, the whole input text   is being generated back and not only the target text!</li> </ul> GPT2Rec<pre><code>GPT2Rec:\n\n\n  # The checkpoint that should be used as starting point of the\n  # fine-tuning process. It can be a model name hosted at hugging face\n  # or a local path # (1)\n  # \n  # Required\n  name_or_path: \"gpt2\"\n\n  # The text to add as prefix to the input part of the prompt fed to the model\n  #\n  # Optional, Default: \"Input: \"\n  input_prefix: \"Input: \"\n\n  # The text to add as prefix to the target part of the prompt fed to the model\n  #\n  # Optional, Default: \"Input: \"\n  target_prefix: \"Target: \"\n\n  # If set to true, this adds a custom EmbeddingLayer to the model which encodes whole word information.\n  # This encoding produces embeddings which have same hidden dimension of the input embeddings, and the two\n  # are summed together. This is basically the implementation of the P5 architecture # (2)\n  #\n  # Optional, Default: false\n  inject_whole_word_embeds: false\n\n  # You can pass any parameter that you would pass to the T5Config when instantiating the model with the\n  # HuggingFace library # (3)\n  CONFIG_PARAM_1: CONFIG_VAL_1\n  CONFIG_PARAM_2: CONFIG_VAL_2\n  ... \n\n  # You can pass any parameter that you would pass to the GenerationConfig when instantiating it with the\n  # HuggingFace library # (4)\n  GEN_PARAM_1: GEN_VAL_1\n  GEN_PARAM_2: GEN_VAL_2\n  ...\n</code></pre> <ol> <li>Check all the available models hosted at HuggingFace!</li> <li>It's the same as described for the T5 model</li> <li>Check all the config parameters that you can pass from the HuggingFace    official documentation</li> <li>Check all the generation parameters that you can pass from the HuggingFace    official documentation</li> </ol>"},{"location":"yaml_usage/available_implementations/available_tasks/","title":"Available Tasks","text":"<p>Each task can have Inference templates and Support templates:</p> <ul> <li>Inference templates are those templates which can be used at inference time and are those upon which   the evaluation is carried out</li> <li>Support templates are those templates which can NOT be used at inference time, since they have intrinsic   assumption which requires the knowledge of the ground truth</li> </ul> <p>Example of inference template:</p> <p> Input Placeholder text Target Placeholder text              Recommend an item for {user_id}                         {target_item}            </p> <p>Example of support template:</p> <p> Input Placeholder text Target Placeholder text              Recommend an item for {user_id}, knowing that a good item to recommend is present among {candidates}                         {target_item}            </p> <p>As you can easily see, the support template requires information that we do not have at inference time: that is, the target item to recommend</p> <p>To specify one of the following tasks the <code>.yaml</code> file, you can simply use its name  (The parsing is not case-sensitive)!</p>"},{"location":"yaml_usage/available_implementations/available_tasks/#sequentialsideinfotask","title":"SequentialSideInfoTask","text":"<p>The SequentialSideInfoTask is built for AmazonDataset: the goal is to predict the next item of the order history of the user. This task has the SideInfo suffix because categories of the items bought by the user are used additional information for the prediction.</p> <p>There are two different support tasks:</p> <ul> <li>Extractive QA: For the specific user_id, given its order history and the categories of each item bought,   select the next element to recommend from a list of candidates</li> <li>Pair Seq: For the specific user_id, given only one element from the order history and its categories,   predict the immediate successor of the element</li> </ul> <p>Inference templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 0              sequential recommendation - {user_id}:              Predict for the user the next element of the following sequence -&gt; {order_history}              The category of each element of the sequence is -&gt; {category_history}                         {target_item}            1              sequential recommendation - {user_id}:              Predict the next element which the user will buy given the following order history -&gt; {order_history}              Each item bought belongs to these categories (in order) -&gt; {category_history}                         {target_item}            2              sequential recommendation - {user_id}:              What is the element that should be recommended to the user knowing that it has bought -&gt; {order_history}              Categories of the items are -&gt; {category_history}                         {target_item}            3              sequential recommendation - {user_id}:              Recommend to the user an item from the catalog given its order history -&gt; {order_history}               Each item of the order history belongs to the following categories (in order) -&gt; {category_history}                         {target_item}            4              sequential recommendation - {user_id}:              This is the order history of the user -&gt; {order_history}              These are the categories of each item -&gt; {category_history}              Please recommend the next element that the user will buy                         {target_item}            5              sequential recommendation - {user_id}:              Please predict what item is best to recommend to the user given its order history -&gt; {order_history}              Categories of each item -&gt; {category_history}               {target_item}            </p> <p>Extractive QA templates</p> <p> Template ID Input Placeholder text Target Placeholder text 6              sequential recommendation - {user_id}:              The user has the following order history -&gt; {order_history}              The categories of each item bought are -&gt; {category_history}              Which item would the user buy next? Select from the following:              {candidate_items}                         {target_item}            7              sequential recommendation - {user_id}:              The user has bought {order_history}, and the categories of those items are {category_history}.              Choose an item to recommend to the user selecting from:              {candidate_items}                         {target_item}            </p> <p>Pair Seq templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 8              sequential recommendation - {user_id}:              The user has recently bought {precedent_item_id} which has the following categories: {categories_precedent_item}              What is the next item to recommend?               {target_item}            9              sequential recommendation - {user_id}:              The latest item bought by the user is {precedent_item_id}.              The categories of that item are {categories_precedent_item}.              Predict which item the user will buy next                         {target_item}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#directsideinfotask","title":"DirectSideInfoTask","text":"<p>The DirectSideInfoTask is built for AmazonDataset: the goal is to predict a good item to recommend for the user. This task has the SideInfo suffix because categories of the items bought by the user are used additional information for the prediction.</p> <p>There are two different support tasks:</p> <ul> <li>Extractive QA: For the specific user_id, given categories liked by the user, select the item to recommend     from a list of candidates</li> </ul> <p>Inference templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 0              direct recommendation - {user_id}:              Pick an item from the catalog knowing that these are the categories the user likes -&gt; {unique_categories_liked}               {target_item}            1              direct recommendation - {user_id}:              Recommend an item to the user. The categories of the items bought by the user are -&gt; {unique_categories_liked}               {target_item}            2              direct recommendation - {user_id}:              What is the item that should be recommended to the user? It likes these categories -&gt; {unique_categories_liked}               {target_item}            3              direct recommendation - {user_id}:              Select an item to present to the user given the categories that it likes -&gt; {unique_categories_liked}               {target_item}            4              direct recommendation - {user_id}:              These are the categories of the items bought by the user -&gt; {unique_categories_liked}              Please recommend an item that the user will buy                         {target_item}            5              direct recommendation - {user_id}:              Please predict what item is best to recommend to the user. The categories that it likes are -&gt; {unique_categories_liked}               {target_item}            </p> <p>Extractive QA templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 6              direct recommendation - {user_id}:              The categories liked by the user are -&gt; {unique_categories_liked}              Which item can interest the user? Select one from the following:              {candidate_items}                         {target_item}            7              direct recommendation - {user_id}:              The user so far has bought items with these categories -&gt; {unique_categories_liked}.              Choose an item to recommend to the user selecting from:              {candidate_items}                         {target_item}            8              direct recommendation - {user_id}:              These are the categories of the items bought by the user -&gt; {unique_categories_liked}.              Predict an item to suggest to the user from the followings:              {candidate_items}                         {target_item}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#ratingpredictiontask","title":"RatingPredictionTask","text":"<p>The RatingPredictionTask is built for AmazonDataset: the goal is to predict the rating that the user would give to an unseen item.</p> <p>Inference templates</p> <p> Template ID Input Placeholder text Target Placeholder text 0              rating prediction - {user_id}:              Average rating of the user -&gt; {avg_rating}              Continue this rating sequence for the user, predicting the rating for {item_id}:              {order_history_w_ratings}                         {target_rating}            1              rating prediction - {user_id}:              Average rating of the user -&gt; {avg_rating}              Predict the rating that the user would give to {item_id}, by considering the following previously bought item and the rating assigned:              {order_history_w_ratings}                         {target_rating}            2              rating prediction - {user_id}:              Predict the score the user would give to {item_id} (in a 1-5 scale).              This is the user order history with associated rating that the user previously gave:              {order_history_w_ratings}              Consider that the average rating of the user is {avg_rating}                         {target_rating}            3              rating prediction - {user_id}:              This is the order history of the user with the associated rating -&gt;              {order_history_w_ratings}              This is the average rating given by the user -&gt; {avg_rating}              Based on that, predict the score (in a 1-5 scale) the user would give to {item_id}                         {target_rating}            4              rating prediction - {user_id}:              Please predict the user, which has an average rating of {avg_rating}, would give to {item_id} based on its order history -&gt;              {order_history_w_ratings}              This is the average rating given by the user -&gt; {avg_rating}              The score should be in a 1-5 scale                         {target_rating}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5-tasks","title":"P5 Tasks","text":"<p>Below there are defined the Rating, Sequential, Direct task prompts from the P5 paper. Each task has its \"Eval\" counterpart, where there are defined the prompts used by the P5 author's for the evaluation phase. Each Eval task has one seen prompt (a prompt used during fine-tuning) and an unseen one.</p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5ratingtask","title":"P5RatingTask","text":"<p>The RatingPredictionTask is built for AmazonDataset: the goal is to predict the rating that the user would give to an unseen item.</p> <p>Inference templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 1-1              Which star rating will user_{user_id} give item_{item_id} ? ( 1 being lowest and 5 being highest )                         {star_rating}            1-2              How will user_{user_id} rate this product : {item_title} ? ( 1 being lowest and 5 being highest )                         {star_rating}            1-5              Predict the user_{user_id} 's preference on item_{item_id} ( {item_title} )               -1               -2               -3               -4               -5                         {star_rating}            1-6              What star rating do you think {user_name} will give item_{item_id} ? ( 1 being lowest and 5 being highest )                         {star_rating}            1-7              How will {user_name} rate this product : {item_title} ? ( 1 being lowest and 5 being highest )                         {star_rating}            </p> <p>Support templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 1-3              Will user_{user_id} give item_{item_id} a {star_rating}-star rating ? ( 1 being lowest and 5 being highest )                         {yes_no}            1-4              Does user_{user_id} like or dislike item_{item_id} ?                         {like_dislike}            1-8              Will {user_name} give a {star_rating}-star rating for {item_title} ? ( 1 being lowest and 5 being highest )                         {yes_no}            1-9              Does {user_name} like or dislike {item_title} ?                         {like_dislike}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5evalratingtask","title":"P5EvalRatingTask","text":"<p> Template ID Input Placeholder text Target Placeholder text 1-6              What star rating do you think {user_name} will give item_{item_id} ? ( 1 being lowest and 5 being highest )                         {star_rating}            1-10              Predict {user_name} 's preference towards {item_title} ( 1 being lowest and 5 being highest )                         {star_rating}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5sequentialtask","title":"P5SequentialTask","text":"<p>The SequentialSideInfoTask is built for AmazonDataset: the goal is to predict the next item of the order history of the user. This task has the SideInfo suffix because categories of the items bought by the user are used additional information for the prediction.</p> <p>Inference templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 2-1              Given the following purchase history of user_{user_id} :               {order_history}               predict next possible item to be purchased by the user ?                         {target_item}            2-2              I find the purchase history list of user_{user_id} :               {order_history}               I wonder what is the next item to recommend to the user . Can you help me decide ?                         {target_item}            2-3              Here is the purchase history list of user_{user_id} :               {order_history}               try to recommend next item to the user                         {target_item}            2-4              Given the following purchase history of {user_name} :               {order_history}               predict next possible item for the user                         {target_item}            2-5              Based on the purchase history of {user_name} :               {order_history}               Can you decide the next item likely to be purchased by the user ?                         {target_item}            2-6              Here is the purchase history of {user_name} :               {order_history}               What to recommend next for the user ?                         {target_item}            </p> <p>Extractive QAs templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 2-7              Here is the purchase history of user_{user_id} :               {order_history}               Select the next possible item likely to be purchased by the user from the following candidates :               {candidate_items}                         {target_item}            2-8              Given the following purchase history of {user_name} :               {order_history}               What to recommend next for the user? Select one from the following items :               {candidate_items}                         {target_item}            2-9              Based on the purchase history of user_{user_id} :               {order_history}               Choose the next possible purchased item from the following candidates :               {candidate_items}                         {target_item}            2-10              I find the purchase history list of {user_name} :               {order_history}               I wonder which is the next item to recommend to the user . Try to select one from the following candidates :               {candidate_items}                         {target_item}            </p> <p>Pairwise prediction templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 2-11              user_{user_id} has the following purchase history :               {order_history}               does the user likely to buy {target_item} next ?                         {yes_no}            2-12              According to {user_name} 's purchase history list :               {order_history}               Predict whether the user will purchase {target_item} next ?                         {yes_no}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5evalsequentialtask","title":"P5EvalSequentialTask","text":"<p> Template ID Input Placeholder text Target Placeholder text 2-3              Here is the purchase history list of user_{user_id} :               {order_history}               try to recommend next item to the user                         {target_item}            2-13              According to the purchase history of {user_name} :               {order_history}               Can you recommend the next possible item to the user ?                         {target_item}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5directtask","title":"P5DirectTask","text":"<p>The P5DirectTask is built for AmazonDataset: the goal is to predict a good item to recommend for the user.</p> <p>Inference templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 5-5              Which item of the following to recommend for {user_name} ?               {candidate_items}                         {target_item}            5-6              Choose the best item from the candidates to recommend for {user_name} ?               {candidate_items}                         {target_item}            5-7              Pick the most suitable item from the following list and recommend to user_{user_id} :               {candidate_items}                         {target_item}            </p> <p>Support templates:</p> <p> Template ID Input Placeholder text Target Placeholder text 5-1              Will user_{user_id} likely to interact with item_{item_id} ?                         {yes_no}            5-2              Shall we recommend item_{item_id} to {user_name} ?                         {yes_no}            5-3              For {user_name}, do you think it is good to recommend {item_title} ?                         {yes_no}            5-4              I would like to recommend some items for user_{user_id} . Is the following item a good choice ?               {item_title}                         {yes_no}            </p>"},{"location":"yaml_usage/available_implementations/available_tasks/#p5evaldirecttask","title":"P5EvalDirectTask","text":"<p> Template ID Input Placeholder text Target Placeholder text 5-5              Which item of the following to recommend for {user_name} ?               {candidate_items}                         {target_item}            5-8              We want to make recommendation for user_{user_id} .  Select the best item from these candidates :               {candidate_items}                         {target_item}            </p>"},{"location":"yaml_usage/yaml_sections/data_section/","title":"Data section","text":"<p>In the data section, the only attribute to specify is the dataset to use along with its parameters, like this:</p> Data section<pre><code>data:\n  DATASET_TO_USE:\n    PARAM1: VAL1\n    PARAM2:\n      - VAL2\n      - VAL3\n    ...\n</code></pre> <p>All parameters of the data section should be defined as attribute of the data mapping</p> <p>Check the available datasets to see which datasets are implemented at the moment and their customizable parameters!</p>"},{"location":"yaml_usage/yaml_sections/eval_section/","title":"Eval section","text":"<p>In the eval section, all parameters related to evaluation are specified, like the eval batch size, the tasks and metrics to use for evaluation, etc.</p> <p>All parameters of the eval section should be defined as attribute of the eval mapping</p> Eval section<pre><code>eval:\n\n  # Mapping between the tasks to evaluate and the metrics to use during evaluation # (1)\n  #\n  # Required\n  eval_tasks:\n    SequentialSideInfo:\n      - hit@1\n      - hit@10\n    RatingPredictionTask:\n      - mae\n      - rmse\n\n  # The batch size to use during evaluation phase. If not specified, it uses\n  # the `eval_batch_size` defined in the 'model' section. If `eval_batch_size` is not\n  # defined in the 'model' section, then it will use the `train_batch_size` defined in \n  # the 'model' section\n  #\n  # Optional, Default: null\n  eval_batch_size: null\n\n  # If set to True, for each task, a latex table storing the results of each template,\n  # will be saved along with the same results saved in CSV Format\n  #\n  # Optional, Default: true\n  create_latex_table: true\n</code></pre> <ol> <li>Be sure to check all available tasks and all available metrics</li> </ol>"},{"location":"yaml_usage/yaml_sections/general_parameters/","title":"General Parameters","text":"General parameters<pre><code># Name of the experiment: it will be used to create the directories\n# in which the trained model and the metrics will be saved, respectively in\n# \"models\" directory and \"reports/metrics\" directory at the root of the repository.\n# In this case:\n# * The model will be stored into \"models/simple_exp\"\n# * The metrics results will be stored into \"reports/metrics/simple_exp\"\n#\n# Required\nexp_name: simple_exp\n\n# Device to use when training the model. Usually is \"cpu\" or \"cuda:0\".\n# Use \"cuda:0\" to use your gpu and speed up the training phase\n#\n# Optional, Default: cuda:0\ndevice: cuda:0\n\n\n# The random state to use for the experiment. For each main phase (data/model/eval),\n# the random state will be initialized to this particular value\n#\n# Optional, Default: 42\nrandom_seed: 42\n\n\n# If set to true the training and evaluation results will be logged to wandb # (1)\n#\n# Optional, Default: false\nlog_wandb: false\n\n# If log_wandb is set to \"true\", you can customize the project to which the run will be logged\n# with this parameter\n#\n# Optional, Default: null\nwandb_project: null\n\n\n# If set to true, only the evaluation phase will be performed. Be sure that the model exists\n# at location \"models/EXPERIMENT_NAME\" # (2)\neval_only: false\n</code></pre> <ol> <li>Be sure to set at least 'WANDB_API_KEY' and 'WANDB_ENTITY' as environment variables like suggested in the official    documentation,    otherwise an exception is raised!</li> <li>EXPERIMENT_NAME is the value of the <code>exp_name</code> property in the .yaml file, in this case it's simple_exp</li> </ol>"},{"location":"yaml_usage/yaml_sections/model_section/","title":"Model section","text":"<p>In the model section, you should define the model to use (and its parameters), along with the training parameters. Note: The model to use must be the first attribute of the <code>model</code> section!</p> Model section<pre><code>model:\n  MODEL_TO_USE:\n    PARAM1: VAL1\n    PARAM2:\n      - VAL2\n      - VAL3\n    ...\n\n\n  # Sequence of tasks to use during the training phase of the model.\n  # Each sample of the train dataset will be applied to one of the followings or\n  # all of them, depending on the `train_task_selection_strat` parameter\n  #\n  # Required\n  train_tasks:\n    - SequentialSideInfoTask\n    - RatingPredictionTask\n\n  # When training according to the multitask paradigm,\n  # there are two different strategies available for choosing the task to apply\n  # for the particular sample of the training set currently processed:\n  # - \"all\" will apply, for the particular sample, ALL training tasks defined # (2)\n  # - \"random\" will apply, for the particular sample, ONE training task among those defined choosen at random # (3)\n  #\n  # Optional, Default: \"all\"\n  train_task_selection_strat: all\n\n  # If set, the validation phase is performed at the end of each epoch of training:\n  # - In this case, the best model will be saved according to the `monitor_metric` value\n  # val_task should be set to one of the available tasks\n  #\n  # Optional, Default: null\n  val_task: null\n\n  # If `val_task` parameter is set, and thus the validation phase is performed,\n  # you could specify the exact template of the `val_task` to use for validation.\n  # If `val_task` is set but this parameter is set to null, then validation is performed\n  # by choosing random templates of the `val_task` to apply to each sample of the\n  # val dataset\n  #\n  # Optional, Default: null\n  val_task_template_id: null\n\n  # Number of epochs to perform during the training phase\n  #\n  # Optional, Default: 10\n  n_epochs: 10\n\n  # If `val_task` is set, and thus the validation phase is performed,\n  # you can change the metric that should be used in order to save the best model\n  #\n  # Optional, Default: loss\n  monitor_metric: loss\n\n  # The batch size to use during the training phase\n  #\n  # Optional, Default: 4\n  train_batch_size: 4\n\n  # If `val_task` parameter is set, and thus the validation phase is performed,\n  # you could change the batch size to use for the validation phase.\n  # If this parameter is set to null, then the `train_batch_size` value will be used\n  # as the `eval_batch_size`\n  eval_batch_size: null\n</code></pre> <p>All parameters of the model section should be defined as attribute of the model mapping</p> <p>Check the available models to see which models are implemented at the moment and their customizable parameters!</p>"}]}